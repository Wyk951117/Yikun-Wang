# 线性回归与逻辑回归
本文试图在复习线性回归和逻辑回归的过程中进行一个记录与总结。
## 线性模型
在进入线性回归之前，首先记录一下线性模型的基本形式。用一句话总结，线性模型是一个试图通过样本不同属性的线性组合进行预测的函数。这类模型往往形式简单、易于建模，同时解释性足够强。经典的线性模型包括线性回归、逻辑回归、线性判别分析（LDA）等。

## 线性回归
* 定义  
给定数据集，线性回归试图从中学得一个针对样本各个属性的线性组合形成的线性模型，用于尽可能准确地预测实值输出标记。  
<center>
<img src="https://latex.codecogs.com/gif.latex?f(x_i)&space;=&space;w^Tx_i&space;&plus;&space;b&space;\&space;,&space;\quad&space;s.t.&space;\&space;f(x_i)&space;\approx&space;y_i" title="f(x_i) = w^Tx_i + b \ , \quad s.t. \ f(x_i) \approx y_i" />
</center>

* 属性的数学表达  
线性回归需要将样本的各个属性都以数学形式表达出来，如果属性本身就是连续属性，则可以直接将其映射到某个取值范围（如 [0,1]）；如果属性为离散值，则可以分为两种情况讨论。如果离散属性存在有序关系，如 “高” 和 “矮”、“青年”、“中年”、“老年”，都可以转化为连续属性{1.0, 0.0}、{1.0, 0.5, 0.0}等。而无序的属性，如颜色属性，最好用k维向量表示，如one-hot向量，其中k即是该属性的类别数。如果在无序属性中不恰当地引入序关系，可能会给后续的距离计算等造成误导。

* 误差度量  
线性回归的经典误差度量方法就是“最小二乘法”，即通过最小化数据集预测值和真实值之间的均方误差来进行模型求解。均方误差的公式为：

<center> <img src="https://latex.codecogs.com/gif.latex?\begin{align*}&space;(w^*,&space;b^*)&space;&=&space;\mathop{argmin}_{(w,b)}&space;\sum_{i=1}^m&space;(f(x_i)&space;-&space;y_i)^2&space;\\&space;&=&space;\mathop{argmin}_{(w,b)}&space;\sum_{i=1}^m&space;(y_i&space;-&space;w^Tx_i&space;-&space;b)^2&space;\end{align*}" title="\begin{align*} (w^*, b^*) &= \mathop{argmin}_{(w,b)} \sum_{i=1}^m (f(x_i) - y_i)^2 \\ &= \mathop{argmin}_{(w,b)} \sum_{i=1}^m (y_i - w^Tx_i - b)^2 \end{align*}" /> </center>

在线性回归中的解释即为试图在样本空间中找到一条直线，使得所有样本到直线上的欧氏距离之和最小。  

* 求解过程  
均方误差显然是可导的，因此线性回归的最小二乘“参数估计”就是通过将误差方程对 *w* 和 *b* 分别进行求导，使导数为零从而得到最优解的闭式解。
在多元线性回归的过程中，通常会将 *b* 吸入 *w* 组成增广矩阵进行求解，则当前的优化目标的形式可以表示为：  

<center>
<img src="https://latex.codecogs.com/gif.latex?\hat{w}^*&space;=&space;\mathop{argmin}_{\hat{w}}&space;(y&space;-&space;X\hat{w})^T(y&space;-&space;X\hat{w})" title="\hat{w}^* = \mathop{argmin}_{\hat{w}} (y - X\hat{w})^T(y - X\hat{w})" />
</center>

 将其转化为误差函数并进行求导可得到：  

<center>
<img src="https://latex.codecogs.com/gif.latex?\frac{\partial{E_{\hat{w}}}}{\partial{\hat{w}}}&space;=&space;2&space;X^T(X\hat{w}&space;-&space;y)" title="\frac{\partial{E_{\hat{w}}}}{\partial{\hat{w}}} = 2 X^T(X\hat{w} - y)" />
 </center>
 
当样本数量大于特征数量，即矩阵 *X* 的转置与其自身的内积为满秩矩阵或正定矩阵时（可以求逆），能够得到该方程的闭式解：  

<center>
<img src="https://latex.codecogs.com/gif.latex?\hat{w}^*&space;=&space;(X^TX)^{-1}X^Ty" title="\hat{w}^* = (X^TX)^{-1}X^Ty" /> 
</center>

则由此得出的线性回归模型为：  

<center>
<img src="https://latex.codecogs.com/gif.latex?f(\hat{x}_i)&space;=&space;\hat{x}_i(X^TX)^{-1}X^Ty" title="f(\hat{x}_i) = \hat{x}_i(X^TX)^{-1}X^Ty" />
 </center>

如果样本数量低于特征数量，会导致该误差方程存在多个解，需要通过学习算法的归纳偏好决定选择哪一个作为最终答案，通常通过引进正则化项来达到这一目的。
线性模型除了可以逼近目标值 *y* 之外，还能逼近其衍生物，即通过联系函数 *g(.)* 将目标值映射到特定空间（如对数），并直接拟合映射后的目标值。这时的线性模型其实可以转化为非线性回归模型，只是保留着线性模型的形式，被称为“广义线性模型”，如对数线性回归模型。  

## 逻辑回归  
逻辑回归也被称为对数几率回归，作为一个货真价实的回归模型，却通常被用于进行分类。这和上文提到的“广义线性模型”有关。逻辑回归通过一个单调可微函数将分类任务的真实标记和线性回归模型的预测值联系到了一起。二分类任务的取值是{0，1}，线性回归模型产生的预测值是整个实数范围，因此需要通过类似“单位阶跃函数”的映射函数将预测值投影到[0，1]范围，并且以0.5作为临界值。由于单位阶跃函数本身并不连续，因此不能直接作为联系函数，因此用对数几率函数替代：

<center>
<img src="https://latex.codecogs.com/gif.latex?y&space;=&space;\frac{1}{1&space;&plus;&space;e^{-(w^Tx&space;&plus;&space;b)}}" title="y = \frac{1}{1 + e^{-(w^Tx + b)}}" />
 </center>

对数几率函数可以将预测值映射到[0, 1]范围内，并且在横坐标0附近很陡峭，可以很好地近似单位阶跃函数。除此之外，对上式进行转化可以得到：

<center>
<img src="https://latex.codecogs.com/gif.latex?\ln&space;\frac{y}{1-y}&space;=&space;w^Tx&space;&plus;&space;b" title="\ln \frac{y}{1-y} = w^Tx + b" />
 </center>

因此还能解释为逻辑回归模型回归的是样本为正例和负例的可能性的比值的对数，也称作对数几率。  
逻辑回归作为一种直接对分类可能性进行建模的模型，无需事先假设数据的分布，因此避免了假设分布不准确带来的问题；同时该模型能够直接给出近似概率，对于需要利用概率辅助决策的任务很有用。最后一点，该回归模型求解的目标函数是任意阶可导的凸函数，下面予以说明。  
如果将上式中的 *y* 视为类后验概率估计 *y=1* 的情况，上式可以改写为：  

<center>
<img src="https://latex.codecogs.com/gif.latex?\ln&space;\frac{p(y=1&space;\|&space;x)}{p(y=0&space;\|&space;x)}&space;=&space;w^Tx&space;&plus;&space;b" title="\ln \frac{p(y=1 \| x)}{p(y=0 \| x)} = w^Tx + b" />
</center>

通过极大似然法最大化这两个似然函数的对数形式，即使每个样本属于其真实分类的概率尽量大。其中对数似然函数为：  

<center>
 <img src="https://latex.codecogs.com/gif.latex?l(w,&space;b)&space;=&space;\sum_{i=1}^m&space;\ln&space;p(y_i&space;\&space;\|&space;\&space;x_i;&space;\&space;w,&space;b)" title="l(w, b) = \sum_{i=1}^m \ln p(y_i \ \| \ x_i; \ w, b)" />
</center> 

同样通过增广矩阵吸收偏置项，上式中的似然项可以重写为：  

<center>
 <img src="https://latex.codecogs.com/gif.latex?p(y_1&space;\&space;\|&space;\&space;x_i;&space;\&space;w,&space;b)&space;=&space;y_i&space;p_1(\hat{x}_i;\beta)&space;&plus;&space;(1&space;-&space;y_i)&space;p_0(\hat{x}_i;\beta)" title="p(y_1 \ \| \ x_i; \ w, b) = y_i p_1(\hat{x}_i;\beta) + (1 - y_i) p_0(\hat{x}_i;\beta)" />
 </center>
 
 将最大化似然函数等价为最小化似然函数的负值，可以得到一个关于 \beta 的高阶可导连续函数，如下所示：  
 
 <center>
 <img src="https://latex.codecogs.com/gif.latex?l(\beta)&space;=&space;\sum_{i&space;=&space;1}^m&space;(-y_i&space;\beta^T&space;\hat{x}_i&space;&plus;&space;\ln&space;(1&space;&plus;&space;e^{\beta^T&space;\hat{x}_i}))" title="l(\beta) = \sum_{i = 1}^m (-y_i \beta^T \hat{x}_i + \ln (1 + e^{\beta^T \hat{x}_i}))" />
</center>

根据凸优化理论，经典的数值优化算法如 GD(SGD)、Newton's method 都可以求得其最优解。
