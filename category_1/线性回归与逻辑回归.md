# 线性回归与逻辑回归
本文试图在复习线性回归和逻辑回归的过程中进行一个记录与总结。
## 线性模型
在进入线性回归之前，首先记录一下线性模型的基本形式。用一句话总结，线性模型是一个试图通过样本不同属性的线性组合进行预测的函数。这类模型往往形式简单、易于建模，同时解释性足够强。经典的线性模型包括线性回归、逻辑回归、线性判别分析（LDA）等。

## 线性回归
* 定义  
给定数据集，线性回归试图从中学得一个针对样本各个属性的线性组合形成的线性模型，用于尽可能准确地预测实值输出标记。  

<img src="https://latex.codecogs.com/gif.latex?f(x_i)&space;=&space;w^Tx_i&space;&plus;&space;b&space;\&space;,&space;\quad&space;s.t.&space;\&space;f(x_i)&space;\approx&space;y_i" title="f(x_i) = w^Tx_i + b \ , \quad s.t. \ f(x_i) \approx y_i" />


* 属性的数学表达  
线性回归需要将样本的各个属性都以数学形式表达出来，如果属性本身就是连续属性，则可以直接将其映射到某个取值范围（如 [0,1]）；如果属性为离散值，则可以分为两种情况讨论。如果离散属性存在有序关系，如 “高” 和 “矮”、“青年”、“中年”、“老年”，都可以转化为连续属性{1.0, 0.0}、{1.0, 0.5, 0.0}等。而无序的属性，如颜色属性，最好用k维向量表示，如one-hot向量，其中k即是该属性的类别数。如果在无序属性中不恰当地引入序关系，可能会给后续的距离计算等造成误导。

* 误差度量  
线性回归的经典误差度量方法就是“最小二乘法”，即通过最小化数据集预测值和真实值之间的均方误差来进行模型求解。均方误差的公式为：

<img src="https://latex.codecogs.com/gif.latex?\begin{align*}&space;(w^*,&space;b^*)&space;&=&space;\mathop{argmin}_{(w,b)}&space;\sum_{i=1}^m&space;(f(x_i)&space;-&space;y_i)^2&space;\\&space;&=&space;\mathop{argmin}_{(w,b)}&space;\sum_{i=1}^m&space;(y_i&space;-&space;w^Tx_i&space;-&space;b)^2&space;\end{align*}" title="\begin{align*} (w^*, b^*) &= \mathop{argmin}_{(w,b)} \sum_{i=1}^m (f(x_i) - y_i)^2 \\ &= \mathop{argmin}_{(w,b)} \sum_{i=1}^m (y_i - w^Tx_i - b)^2 \end{align*}" />

在线性回归中的解释即为试图在样本空间中找到一条直线，使得所有样本到直线上的欧氏距离之和最小。  

* 求解过程  
均方误差显然是可导的，因此线性回归的最小二乘“参数估计”就是通过将误差方程对 *w* 和 *b* 分别进行求导，使导数为零从而得到最优解的闭式解。
在多元线性回归的过程中，通常会将 *b* 吸入 *w* 组成增广矩阵进行求解，则当前的优化目标的形式可以表示为：  

<img src="https://latex.codecogs.com/gif.latex?\hat{w}^*&space;=&space;\mathop{argmin}_{\hat{w}}&space;(y&space;-&space;X\hat{w})^T(y&space;-&space;X\hat{w})" title="\hat{w}^* = \mathop{argmin}_{\hat{w}} (y - X\hat{w})^T(y - X\hat{w})" />

 将其转化为误差函数并进行求导可得到：  
 
<div align=center> <img src="https://latex.codecogs.com/gif.latex?\frac{\partial{E_{\hat{w}}}}{\partial{\hat{w}}}&space;=&space;2&space;X^T(X\hat{w}&space;-&space;y)" title="\frac{\partial{E_{\hat{w}}}}{\partial{\hat{w}}} = 2 X^T(X\hat{w} - y)" /> </div>
 
当样本数量大于特征数量，即矩阵 *X* 的转置与其自身的内积为满秩矩阵或正定矩阵时（可以求逆），能够得到该方程的闭式解：  

<img src="https://latex.codecogs.com/gif.latex?\hat{w}^*&space;=&space;(X^TX)^{-1}X^Ty" title="\hat{w}^* = (X^TX)^{-1}X^Ty" /> 

则由此得出的线性回归模型为：  

<div align=center>
<img src="https://latex.codecogs.com/gif.latex?f(\hat{x}_i)&space;=&space;\hat{x}_i(X^TX)^{-1}X^Ty" title="f(\hat{x}_i) = \hat{x}_i(X^TX)^{-1}X^Ty" />
 </div>

如果样本数量低于特征数量，会导致该误差方程存在多个解，需要通过学习算法的归纳偏好决定选择哪一个作为最终答案，通常通过引进正则化项来达到这一目的。
线性模型除了可以逼近目标值 *y* 之外，还能逼近其衍生物，即通过联系函数 *g(.)* 将目标值映射到特定空间（如对数），并直接拟合映射后的目标值。这时的线性模型其实可以转化为非线性回归模型，只是保留着线性模型的形式，被称为“广义线性模型”，如对数线性回归模型。  

## 逻辑回归  
逻辑回归也被称为对数几率回归，作为一个货真价实的回归模型，却通常被用于进行分类。这和上文提到的“广义线性模型”有关。逻辑回归通过一个单调可微函数将分类任务的真实标记和线性回归模型的预测值联系到了一起。二分类任务的取值是{0，1}，线性回归模型产生的预测值是整个实数范围，因此需要通过类似“单位阶跃函数”的映射函数将预测值投影到[0，1]范围，并且以0.5作为临界值。  

