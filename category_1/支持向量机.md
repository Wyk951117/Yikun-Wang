# 支持向量机  
### 出发点：  
在线性分类器的诸多超平面中，选择离两个类别的分隔界都尽量远的“正中间”的超平面，从而获取一个更加鲁棒，泛化能力更强的分类器。（鲁棒指该分类器能够更好地“容忍”由噪声等因素造成的局部扰动；泛化能力更强对应可能出现的训练集测试集分布不一致的情况）  
### 推导过程：  
首先，划分超平面在样本空间中可以被表示为如下的线性方程：  
  <center> <img src="https://latex.codecogs.com/gif.latex?w^Tx&space;&plus;&space;b&space;=&space;0" title="w^Tx + b = 0" /> </center>
在这个基础上，样本空间中的任意点到这个超平面的距离可以被表示为：  
 <center> <img src="https://latex.codecogs.com/gif.latex?r&space;=&space;\frac{|w^Tx&space;&plus;&space;b|}{\left\|w\right\|}" title="r = \frac{|w^Tx + b|}{\left\|w\right\|}" /> </center>    
 两个类别中距离超平面最近的点都被称为“支持向量”，因为是它们决定了超平面的方向（法向量）和相对原点的位移（偏置项）。两类的支持向量到超平面的距离之和可以表示为：  
 <center> <img src="https://latex.codecogs.com/gif.latex?\gamma&space;=&space;\frac{2}{\left\|w\right\|}" title="\gamma = \frac{2}{\left\|w\right\|}" /> </center>  
 之所以可以将分子直接定为2，是因为法向量的绝对值是没有意义的，总能找到使得这个式子取值为0的w值以及对应的b值。  
 既然已经找到了两类和超平面之间间隔的表达式，下一步就是最大化这个间隔：  
 <center> <img src="https://latex.codecogs.com/gif.latex?\underset{w,&space;b}{max}&space;\&space;\frac{2}{\left\|w\right\|}" title="\underset{w, b}{max} \ \frac{2}{\left\|w\right\|}" /> </center>  
 <center> <img src="https://latex.codecogs.com/gif.latex?s.t.&space;\&space;\&space;y_i(w^Tx_i&space;&plus;&space;b)&space;\geq&space;1,&space;\&space;\&space;\&space;i&space;=&space;1,&space;2,&space;...,&space;m" title="s.t. \ \ y_i(w^Tx_i + b) \geq 1, \ \ \ i = 1, 2, ..., m" /> </center>  
 其中最大化法向量绝对值的倒数的操作显然可以转化为最小化其本身的操作：  
 <center> <img src="https://latex.codecogs.com/gif.latex?\underset{w,&space;b}{min}&space;\&space;\left\|&space;w&space;\right\|&space;^&space;2" title="\underset{w, b}{min} \ \left\| w \right\| ^ 2" /> </center>  
 以上便是支持向量机的基本形式了。      
 
   
 ### 问题的求解：  
 有了优化的表达式之后，需要做的便是求解。这是一个凸二次优化问题，可以通过拉格朗日乘子法得到其对偶问题进行求解。对上述约束添加拉格朗日乘子，则该问题的拉格朗日函数可以写为：  
 <center> <img src="https://latex.codecogs.com/gif.latex?L(w,&space;b,&space;\alpha)&space;=&space;\frac{1}{2}\left\|&space;w&space;\right\|^2&space;&plus;&space;\sum_{i=1}^m&space;\alpha_i(1&space;-&space;y_i(w^Tx_i&space;&plus;&space;b))" title="L(w, b, \alpha) = \frac{1}{2}\left\| w \right\|^2 + \sum_{i=1}^m \alpha_i(1 - y_i(w^Tx_i + b))" /> </center>  
 令该函数对w和b的偏导分别为零可以得到：  
 <center> <img src="https://latex.codecogs.com/gif.latex?w&space;=&space;\sum_{i&space;=&space;1}^m&space;\alpha_iy_ix_i&space;\\&space;\&space;\&space;\\\\&space;0&space;=&space;\sum_{i=1}^m&space;\alpha_iy_i" title="w = \sum_{i = 1}^m \alpha_iy_ix_i \\ \ \ \\\\ 0 = \sum_{i=1}^m \alpha_iy_i" /> </center>  
 可以解得对偶问题：  
 <center> <img src="https://latex.codecogs.com/gif.latex?\underset{\alpha}{max}&space;\sum_{i=1}^m&space;\alpha_i&space;-&space;\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_j&space;y_iy_jx_i^Tx_j" title="\underset{\alpha}{max} \sum_{i=1}^m \alpha_i - \frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_j y_iy_jx_i^Tx_j" /> </center>  
 <center> <img src="https://latex.codecogs.com/gif.latex?s.t.&space;\&space;\&space;\sum_{i=1}^m\alpha_iy_i&space;=&space;0,&space;\and&space;\&space;\&space;\alpha_i&space;\geq&space;0,\&space;\&space;i=1,2,3,...,m" title="s.t. \ \ \sum_{i=1}^m\alpha_iy_i = 0, \and \ \ \alpha_i \geq 0,\ \ i=1,2,3,...,m" /> </center>  
 解出该约束条件后就能够得到最终超平面的表达式：  
 <center> <img src="https://latex.codecogs.com/gif.latex?\begin{align*}&space;f(x)&space;&=&space;w^T&space;&plus;&space;b&space;\\&space;&=&space;\sum_{i=1}^m&space;\alpha_iy_ix_i^Tx&space;&plus;&space;b&space;\end{align*}" title="\begin{align*} f(x) &= w^T + b \\ &= \sum_{i=1}^m \alpha_iy_ix_i^Tx + b \end{align*}" /> </center>  
   
 * 求解过程：  
 前述的对偶问题表达式是一个标准的二次规划问题，可以用通用的算法来解决，但是会遍历所有的训练样本。为了快速求解，人们通常使用SMO算法进行求解，即先固定任意alpha之外的所有参数，求其极值。通常选取一对间隔最大的样本点，用其中一个的alpha表示另一个的alpha，从而使得目标函数直观上变化更大。计算偏置b通常通过所有支持向量的平均值来完成。  
 
 ### 再谈“支持向量”  
 上述求解过程由于存在不等式约束，因此需要满足KKT条件，即:  
 <center> <img src="https://latex.codecogs.com/gif.latex?\left\{\begin{matrix}&space;\alpha_i&space;\geq&space;0;\\&space;y_if(x_i)&space;-&space;1&space;\geq&space;1;\\&space;\alpha_i(y_if(x_i)&space;-&space;1)&space;=&space;0;&space;\end{matrix}\right." title="\left\{\begin{matrix} \alpha_i \geq 0;\\ y_if(x_i) - 1 \geq 1;\\ \alpha_i(y_if(x_i) - 1) = 0; \end{matrix}\right." /> </center>  
 不难看出，样本集中的任一点或是不出现在超平面的表达式中，或是刚好在最大间隔边界上，即为一个“支持向量”。  
 
 ### 核方法：  
 当在原始空间中，两类样本出现线性不可分的情况时，可以通过将样本从原始空间映射到一个更高维的特征空间中，使得样本在这个特征空间中线性可分。关于支持向量机的核方法，存在一个特质，即只要原始空间是有限维的，就一定能找到一个对应的高维特征空间使得样本线性可分。将一系列原公式中的x替换为对应的高维特征向量，可以得到新的约束项表示：  
 <center> <img src="https://latex.codecogs.com/gif.latex?\underset{w,&space;b}{min}&space;\&space;\frac{1}{2}&space;\left\|w\right\|^2" title="\underset{w, b}{min} \ \frac{1}{2} \left\|w\right\|^2" />  </center>  
 <center> <img src="https://latex.codecogs.com/gif.latex?s.t.&space;\&space;\&space;y_i(w^T\phi(x_i)&space;&plus;&space;b)&space;\geq&space;1&space;\&space;,&space;\&space;\&space;i&space;=&space;1,&space;2,&space;...,&space;m" title="s.t. \ \ y_i(w^T\phi(x_i) + b) \geq 1 \ , \ \ i = 1, 2, ..., m" />  </center>  
 
因此，该线性分类器的闭式解变为了如下形式：  
<center> <img src="https://latex.codecogs.com/gif.latex?f(x)&space;=&space;w^T\phi(x)&space;&plus;&space;b&space;=&space;\sum_{i&space;=&space;1}^m\alpha_iy_i\phi(x_i)^T\phi(x)&space;&plus;&space;b" title="f(x) = w^T\phi(x) + b = \sum_{i = 1}^m\alpha_iy_i\phi(x_i)^T\phi(x) + b" />  </center>
其中，由于高维特征空间可能有很多个维度，两个高维向量的乘积可能很难计算，为了避免计算量过大，可以将两个高维特征的乘积用一个函数来表示，即为核函数：  
<center> <img src="https://latex.codecogs.com/gif.latex?k(x_i,&space;x_j)&space;=&space;\left\langle&space;\phi(x_i),&space;\phi(x_j)\right\rangle&space;=&space;\phi(x_i)^T\phi(x_j)" title="k(x_i, x_j) = \left\langle \phi(x_i), \phi(x_j)\right\rangle = \phi(x_i)^T\phi(x_j)" /> </center>  
因此模型的最优解又可以通过训练样本的核函数展开，即“支持向量展式”：  
<center> <img src="https://latex.codecogs.com/gif.latex?f(x)&space;=&space;\sum_{i=1}^m&space;\alpha_iy_ik(x_i,&space;x)&space;&plus;&space;b" title="f(x) = \sum_{i=1}^m \alpha_iy_ik(x_i, x) + b" />  </center>  
任何半正定的矩阵都可以作为核函数使用，每一个核函数都隐式地定义了一个“再生核希尔伯特空间”，因此核函数的选择对支持向量机十分重要。常用的核函数有线性核、多项式核、高斯核。任意两个核函数的线性组合、内积也都是核函数。
