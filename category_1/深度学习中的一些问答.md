# 深度学习中的一些问答  
## 深度神经网络中的激活函数  
#### 常用激活函数及其导数  
1. Sigmoid激活函数：  
  <center> <img src="https://latex.codecogs.com/gif.latex?f(z)&space;=&space;\frac{1}{1&space;&plus;&space;\exp(-z)}" title="f(z) = \frac{1}{1 + \exp(-z)}" /> </center>  
  对应的导数形式为：  
  <center> <img src="https://latex.codecogs.com/gif.latex?f^{'}(z)&space;=&space;f(z)(1-f(z))" title="f^{'}(z) = f(z)(1-f(z))" /> </center>  
2. Tanh激活函数：  
  <center> <img src="https://latex.codecogs.com/gif.latex?f(z)&space;=&space;\frac{e^{z}&space;-&space;e^{-z}}{e^{z}&space;&plus;&space;e^{-z}}" title="f(z) = \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}" /> </center>  
  对应的导数形式为：  
  <center> <img src="https://latex.codecogs.com/gif.latex?f^{'}(z)&space;=&space;1&space;-&space;(f(z))^2" title="f^{'}(z) = 1 - (f(z))^2" /> </center>  
3. ReLU激活函数：  
  <center> <img src="https://latex.codecogs.com/gif.latex?f(z)&space;=&space;\max{(0,&space;z)}" title="f(z) = \max{(0, z)}" /> </center>  
  对应的导数形式为：  
  <center> <img src="https://latex.codecogs.com/gif.latex?f^{'}(z)&space;=&space;1&space;\&space;if&space;\&space;z&space;>&space;0&space;\&space;else&space;\&space;0" title="f^{'}(z) = 1 \ if \ z > 0 \ else \ 0" /> </center>  

* 为什么Sigmoid函数和Tanh函数会导致梯度消失的现象：  
  从Sigmoid激活函数的公式容易发现，当z趋近于正无穷的时候，Sigmoid函数中的指数项趋近于0，因此Sigmoid函数本身趋近于1，其导数趋近于0；同理可知当z趋近于负无穷的时候，其导数的另一项趋近于0；  
  而仔细观察Tanh函数可以发现，Tanh函数其实只是Sigmoid函数的平移。  
* ReLU激活函数相对于另外两种激活函数的优点及其局限性：  
  1. 计算简便：从计算的角度来说，Sigmoid函数和Tanh函数的计算均需要有指数的参与，复杂度比较高，而ReLU函数的计算只需要一个阈值；  
  2. 宽广的激活边界： ReLU的非饱和性可以有效地解决另外两个函数梯度消失的问题，从而提供相对宽的激活边界；  
  3. 一定程度的稀疏表达能力：ReLU函数的单侧抑制提供了网络的稀疏表达能力。  
  局限性：  
  1. 神经元的死亡问题：负梯度被置0后将永远不会被激活，也不会对数据产生任何影响，大量的参数死亡会导致训练失败；
  解决办法：  
  1. Leaky ReLU：在z小于0时以一个斜率较小的线性函数代替直接变为0的操作（但是需要人为选择）；  
  2. PReLU，将上述线性函数饿斜率作为网络中的一个可学习参数，在训练过程中更新，避免人工调参；  
  3. Random ReLU：训练时随机采样，测试时固定。  

## 神经网络的一些训练技巧：  
* 解决过拟合的一些方法：  
  * 数据集增强（Data augmentation）
  * 正则化（Regularization）
  * 模型集成（Model Emsemble）
  * Dropout  
* Dropout抑制过拟合的原理：  
  在深度网络训练的过程中，以一定概率随机关闭一些神经元节点，让其暂时不更新，但是这种关闭是临时的。这样做的结果是，不同的批次数据在通过网络的过程中所面对的网络结果并不是完全一致的。有人认为这是能够类比于Bagging方法的应用于深度网络模型的模型集成算法，即共享部分权重，由独享在其他时候被关闭的那部分权值的多个模型，以N个神经元的网络为例，则是2^N个模型的集成。  
  前向传播过程中，每个神经元的系数都服从概率为p的伯努利分布，代表每个神经元是否会被丢弃。如果取值为0，该神经元不会参与此后的梯度计算和误差传播。  
* 批量归一化的基本动机和原理：  
  * 在神经网络的训练过程中，我们通常需要在训练开始的时候对数据进行归一化处理，从而有利于网络学习数据的分布，加快收敛。在应用mini-batch等小批次训练的方法后，每一次送入网络的数据分布都不相同，从而使得之前的归一化失效，所以需要针对每一批数据进行归一化。传统的归一化是直接以均值为0，标准差为1进行归一化，集对每一个神经元的原始输入都减去均值并除以标准差。  
  BN通过对数据的分布进行额外的约束，增强了模型的泛化能力，但是同时也降低了模型的拟合能力，例如Sigmoid函数的0均值1标准差区间几乎都是线性变换，破坏了之前学到的特征分布。为此，实现过程中又对于归一化后的内容引入了系数和偏置，均通过网络进行学习。
